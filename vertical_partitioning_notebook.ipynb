{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Latency Prediction: Vertical Partitioning Approach\n",
    "\n",
    "This notebook demonstrates the vertical partitioning strategy for network latency prediction. We'll split features into infrastructure-related (Model A) and user behavior-related (Model B) subsets, train specialized models, and fuse their predictions.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading and Exploration](#data-loading)\n",
    "2. [Data Preprocessing](#preprocessing)\n",
    "3. [Vertical Feature Partitioning](#vertical-partitioning)\n",
    "4. [Model Training](#model-training)\n",
    "5. [Model Fusion](#model-fusion)\n",
    "6. [Performance Evaluation](#evaluation)\n",
    "7. [Results Analysis](#analysis)\n",
    "8. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project modules\n",
    "from data_processing.data_loader import DataLoader\n",
    "from data_processing.feature_engineering import FeatureEngineer\n",
    "from models.infrastructure_model import InfrastructureModel\n",
    "from models.user_behavior_model import UserBehaviorModel\n",
    "from models.fusion_model import FusionModel\n",
    "from models.monolithic_model import MonolithicModel\n",
    "from evaluation.model_evaluator import ModelEvaluator\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "DATA_FILE = 'test_data.xlsx'\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data-loading\"></a>\n",
    "## 1. Data Loading and Exploration\n",
    "\n",
    "Let's start by loading our network latency dataset and exploring its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "raw_data = data_loader.load_dataset(DATA_FILE)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {raw_data.shape}\")\n",
    "print(f\"Columns: {list(raw_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(raw_data.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_values = raw_data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for data exploration\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Network Latency Dataset - Exploratory Data Analysis', fontsize=16)\n",
    "\n",
    "# Target variable distribution\n",
    "axes[0, 0].hist(raw_data['Latency (ms)'], bins=30, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Latency Distribution')\n",
    "axes[0, 0].set_xlabel('Latency (ms)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Signal Strength distribution\n",
    "axes[0, 1].hist(raw_data['Signal Strength (dBm)'], bins=30, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Signal Strength Distribution')\n",
    "axes[0, 1].set_xlabel('Signal Strength (dBm)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Network Traffic distribution\n",
    "axes[0, 2].hist(raw_data['Network Traffic (MB)'], bins=30, alpha=0.7, color='salmon')\n",
    "axes[0, 2].set_title('Network Traffic Distribution')\n",
    "axes[0, 2].set_xlabel('Network Traffic (MB)')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "# User Count distribution\n",
    "axes[1, 0].hist(raw_data['User Count'], bins=30, alpha=0.7, color='gold')\n",
    "axes[1, 0].set_title('User Count Distribution')\n",
    "axes[1, 0].set_xlabel('User Count')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Device Type distribution\n",
    "device_counts = raw_data['Device Type'].value_counts()\n",
    "axes[1, 1].bar(device_counts.index, device_counts.values, alpha=0.7, color='plum')\n",
    "axes[1, 1].set_title('Device Type Distribution')\n",
    "axes[1, 1].set_xlabel('Device Type')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Location Type distribution\n",
    "location_counts = raw_data['Location Type'].value_counts()\n",
    "axes[1, 2].bar(location_counts.index, location_counts.values, alpha=0.7, color='lightcoral')\n",
    "axes[1, 2].set_title('Location Type Distribution')\n",
    "axes[1, 2].set_xlabel('Location Type')\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "# Select only numeric columns for correlation\n",
    "numeric_cols = ['Signal Strength (dBm)', 'Network Traffic (MB)', 'Latency (ms)', 'User Count']\n",
    "correlation_matrix = raw_data[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation with Latency:\")\n",
    "latency_corr = correlation_matrix['Latency (ms)'].sort_values(key=abs, ascending=False)\n",
    "for feature, corr in latency_corr.items():\n",
    "    if feature != 'Latency (ms)':\n",
    "        print(f\"{feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "## 2. Data Preprocessing\n",
    "\n",
    "Now let's preprocess the data to prepare it for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data structure\n",
    "print(\"Validating data structure...\")\n",
    "is_valid = data_loader.validate_data(raw_data)\n",
    "\n",
    "if not is_valid:\n",
    "    validation_errors = data_loader.get_validation_errors()\n",
    "    print(\"Validation issues found:\")\n",
    "    for error in validation_errors:\n",
    "        print(f\"  - {error}\")\n",
    "else:\n",
    "    print(\"Data validation passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "print(\"Preprocessing data...\")\n",
    "processed_data = data_loader.preprocess_data(raw_data)\n",
    "\n",
    "print(f\"Original data shape: {raw_data.shape}\")\n",
    "print(f\"Processed data shape: {processed_data.shape}\")\n",
    "print(f\"Rows removed during preprocessing: {len(raw_data) - len(processed_data)}\")\n",
    "\n",
    "# Display processed data summary\n",
    "print(\"\\nProcessed data summary:\")\n",
    "processed_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "print(\"Splitting data into train/test sets...\")\n",
    "train_data, test_data = train_test_split(\n",
    "    processed_data, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {train_data.shape}\")\n",
    "print(f\"Testing set shape: {test_data.shape}\")\n",
    "print(f\"Training set size: {len(train_data)} samples\")\n",
    "print(f\"Testing set size: {len(test_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vertical-partitioning\"></a>\n",
    "## 3. Vertical Feature Partitioning\n",
    "\n",
    "In vertical partitioning, we split features into two groups:\n",
    "- **Model A (Infrastructure)**: Signal Strength, Network Traffic\n",
    "- **Model B (User Behavior)**: User Count, Device Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "# Split features vertically for training data\n",
    "print(\"Performing vertical feature partitioning...\")\n",
    "train_infra, train_user = feature_engineer.split_vertical_features(\n",
    "    train_data, include_target=True\n",
    ")\n",
    "\n",
    "# Split features vertically for testing data\n",
    "test_infra, test_user = feature_engineer.split_vertical_features(\n",
    "    test_data, include_target=True\n",
    ")\n",
    "\n",
    "print(\"Vertical partitioning completed!\")\n",
    "print(f\"\\nInfrastructure features (Model A):\")\n",
    "print(f\"  Training shape: {train_infra.shape}\")\n",
    "print(f\"  Testing shape: {test_infra.shape}\")\n",
    "print(f\"  Columns: {list(train_infra.columns)}\")\n",
    "\n",
    "print(f\"\\nUser Behavior features (Model B):\")\n",
    "print(f\"  Training shape: {train_user.shape}\")\n",
    "print(f\"  Testing shape: {test_user.shape}\")\n",
    "print(f\"  Columns: {list(train_user.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the feature partitioning\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Vertical Feature Partitioning Visualization', fontsize=16)\n",
    "\n",
    "# Infrastructure features vs Latency\n",
    "axes[0, 0].scatter(train_infra['Signal Strength (dBm)'], train_infra['Latency (ms)'], \n",
    "                   alpha=0.6, color='blue')\n",
    "axes[0, 0].set_title('Signal Strength vs Latency')\n",
    "axes[0, 0].set_xlabel('Signal Strength (dBm)')\n",
    "axes[0, 0].set_ylabel('Latency (ms)')\n",
    "\n",
    "axes[0, 1].scatter(train_infra['Network Traffic (MB)'], train_infra['Latency (ms)'], \n",
    "                   alpha=0.6, color='green')\n",
    "axes[0, 1].set_title('Network Traffic vs Latency')\n",
    "axes[0, 1].set_xlabel('Network Traffic (MB)')\n",
    "axes[0, 1].set_ylabel('Latency (ms)')\n",
    "\n",
    "# User behavior features vs Latency\n",
    "axes[1, 0].scatter(train_user['User Count'], train_user['Latency (ms)'], \n",
    "                   alpha=0.6, color='red')\n",
    "axes[1, 0].set_title('User Count vs Latency')\n",
    "axes[1, 0].set_xlabel('User Count')\n",
    "axes[1, 0].set_ylabel('Latency (ms)')\n",
    "\n",
    "# Device Type vs Latency (boxplot)\n",
    "device_types = train_user['Device Type'].unique()\n",
    "device_latencies = [train_user[train_user['Device Type'] == dt]['Latency (ms)'].values \n",
    "                   for dt in device_types]\n",
    "axes[1, 1].boxplot(device_latencies, labels=device_types)\n",
    "axes[1, 1].set_title('Device Type vs Latency')\n",
    "axes[1, 1].set_xlabel('Device Type')\n",
    "axes[1, 1].set_ylabel('Latency (ms)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model-training\"></a>\n",
    "## 4. Model Training\n",
    "\n",
    "Now we'll train our specialized models:\n",
    "- **Model A**: Infrastructure Model (Signal Strength + Network Traffic)\n",
    "- **Model B**: User Behavior Model (User Count + Device Type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training Model A (Infrastructure Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for Model A (Infrastructure)\n",
    "print(\"Training Model A (Infrastructure Model)...\")\n",
    "X_train_infra, y_train_infra = feature_engineer.prepare_infrastructure_features(\n",
    "    train_infra, fit_scaler=True\n",
    ")\n",
    "X_test_infra, y_test_infra = feature_engineer.prepare_infrastructure_features(\n",
    "    test_infra, fit_scaler=False\n",
    ")\n",
    "\n",
    "print(f\"Infrastructure training features shape: {X_train_infra.shape}\")\n",
    "print(f\"Infrastructure testing features shape: {X_test_infra.shape}\")\n",
    "\n",
    "# Train Infrastructure Model\n",
    "infrastructure_model = InfrastructureModel(random_state=RANDOM_STATE)\n",
    "infrastructure_model.train(X_train_infra, y_train_infra)\n",
    "\n",
    "print(\"Model A training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model A\n",
    "pred_infra_train = infrastructure_model.predict(X_train_infra)\n",
    "pred_infra_test = infrastructure_model.predict(X_test_infra)\n",
    "\n",
    "# Calculate metrics\n",
    "evaluator = ModelEvaluator()\n",
    "infra_train_metrics = evaluator.calculate_metrics(y_train_infra, pred_infra_train)\n",
    "infra_test_metrics = evaluator.calculate_metrics(y_test_infra, pred_infra_test)\n",
    "\n",
    "print(\"Model A (Infrastructure) Performance:\")\n",
    "print(f\"Training - MAE: {infra_train_metrics['MAE']:.3f}, RMSE: {infra_train_metrics['RMSE']:.3f}, R²: {infra_train_metrics['R2_Score']:.3f}\")\n",
    "print(f\"Testing  - MAE: {infra_test_metrics['MAE']:.3f}, RMSE: {infra_test_metrics['RMSE']:.3f}, R²: {infra_test_metrics['R2_Score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Model B (User Behavior Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for Model B (User Behavior)\n",
    "print(\"Training Model B (User Behavior Model)...\")\n",
    "\n",
    "# Prepare user behavior features - use raw features for UserBehaviorModel\n",
    "user_count_train = train_user['User Count'].values.reshape(-1, 1)\n",
    "device_type_train = train_user['Device Type'].values.reshape(-1, 1)\n",
    "X_train_user = np.hstack([user_count_train, device_type_train])\n",
    "y_train_user = train_user['Latency (ms)'].values\n",
    "\n",
    "user_count_test = test_user['User Count'].values.reshape(-1, 1)\n",
    "device_type_test = test_user['Device Type'].values.reshape(-1, 1)\n",
    "X_test_user = np.hstack([user_count_test, device_type_test])\n",
    "y_test_user = test_user['Latency (ms)'].values\n",
    "\n",
    "print(f\"User behavior training features shape: {X_train_user.shape}\")\n",
    "print(f\"User behavior testing features shape: {X_test_user.shape}\")\n",
    "\n",
    "# Train User Behavior Model\n",
    "user_behavior_model = UserBehaviorModel(random_state=RANDOM_STATE)\n",
    "user_behavior_model.train(X_train_user, y_train_user)\n",
    "\n",
    "print(\"Model B training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model B\n",
    "pred_user_train = user_behavior_model.predict(X_train_user)\n",
    "pred_user_test = user_behavior_model.predict(X_test_user)\n",
    "\n",
    "# Calculate metrics\n",
    "user_train_metrics = evaluator.calculate_metrics(y_train_user, pred_user_train)\n",
    "user_test_metrics = evaluator.calculate_metrics(y_test_user, pred_user_test)\n",
    "\n",
    "print(\"Model B (User Behavior) Performance:\")\n",
    "print(f\"Training - MAE: {user_train_metrics['MAE']:.3f}, RMSE: {user_train_metrics['RMSE']:.3f}, R²: {user_train_metrics['R2_Score']:.3f}\")\n",
    "print(f\"Testing  - MAE: {user_test_metrics['MAE']:.3f}, RMSE: {user_test_metrics['RMSE']:.3f}, R²: {user_test_metrics['R2_Score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Individual Model Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize individual model predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Individual Model Predictions vs Actual Values', fontsize=16)\n",
    "\n",
    "# Model A predictions\n",
    "axes[0].scatter(y_test_infra, pred_infra_test, alpha=0.6, color='blue')\n",
    "axes[0].plot([y_test_infra.min(), y_test_infra.max()], \n",
    "             [y_test_infra.min(), y_test_infra.max()], 'r--', lw=2)\n",
    "axes[0].set_title(f'Model A (Infrastructure)\\nR² = {infra_test_metrics[\"R2_Score\"]:.3f}')\n",
    "axes[0].set_xlabel('Actual Latency (ms)')\n",
    "axes[0].set_ylabel('Predicted Latency (ms)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model B predictions\n",
    "axes[1].scatter(y_test_user, pred_user_test, alpha=0.6, color='green')\n",
    "axes[1].plot([y_test_user.min(), y_test_user.max()], \n",
    "             [y_test_user.min(), y_test_user.max()], 'r--', lw=2)\n",
    "axes[1].set_title(f'Model B (User Behavior)\\nR² = {user_test_metrics[\"R2_Score\"]:.3f}')\n",
    "axes[1].set_xlabel('Actual Latency (ms)')\n",
    "axes[1].set_ylabel('Predicted Latency (ms)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model-fusion\"></a>\n",
    "## 5. Model Fusion\n",
    "\n",
    "Now we'll combine the predictions from both models using a fusion strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare combined features for fusion model\n",
    "def prepare_fusion_features(data):\n",
    "    \"\"\"Prepare features for fusion model.\"\"\"\n",
    "    required_cols = ['Signal Strength (dBm)', 'Network Traffic (MB)', 'User Count', 'Device Type']\n",
    "    features = []\n",
    "    features.append(data['Signal Strength (dBm)'].values)\n",
    "    features.append(data['Network Traffic (MB)'].values)\n",
    "    features.append(data['User Count'].values)\n",
    "    features.append(data['Device Type'].values)\n",
    "    return np.column_stack(features)\n",
    "\n",
    "print(\"Training Fusion Model...\")\n",
    "X_train_combined = prepare_fusion_features(train_data)\n",
    "y_train_combined = train_data['Latency (ms)'].values\n",
    "\n",
    "X_test_combined = prepare_fusion_features(test_data)\n",
    "y_test_combined = test_data['Latency (ms)'].values\n",
    "\n",
    "print(f\"Combined training features shape: {X_train_combined.shape}\")\n",
    "print(f\"Combined testing features shape: {X_test_combined.shape}\")\n",
    "\n",
    "# Train fusion model\n",
    "fusion_model = FusionModel(\n",
    "    fusion_strategy='weighted_average',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "fusion_model.train(X_train_combined, y_train_combined)\n",
    "\n",
    "print(\"Fusion model training completed!\")\n",
    "\n",
    "# Get fusion weights\n",
    "weights = fusion_model.get_fusion_weights()\n",
    "if weights:\n",
    "    print(f\"\\nFusion weights:\")\n",
    "    print(f\"  Infrastructure Model: {weights['infrastructure']:.3f}\")\n",
    "    print(f\"  User Behavior Model: {weights['user_behavior']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Fusion Model\n",
    "pred_fusion_train = fusion_model.predict(X_train_combined)\n",
    "pred_fusion_test = fusion_model.predict(X_test_combined)\n",
    "\n",
    "# Calculate metrics\n",
    "fusion_train_metrics = evaluator.calculate_metrics(y_train_combined, pred_fusion_train)\n",
    "fusion_test_metrics = evaluator.calculate_metrics(y_test_combined, pred_fusion_test)\n",
    "\n",
    "print(\"Fusion Model Performance:\")\n",
    "print(f\"Training - MAE: {fusion_train_metrics['MAE']:.3f}, RMSE: {fusion_train_metrics['RMSE']:.3f}, R²: {fusion_train_metrics['R2_Score']:.3f}\")\n",
    "print(f\"Testing  - MAE: {fusion_test_metrics['MAE']:.3f}, RMSE: {fusion_test_metrics['RMSE']:.3f}, R²: {fusion_test_metrics['R2_Score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Baseline Monolithic Model\n",
    "\n",
    "Let's train a baseline model using all features together for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline monolithic model\n",
    "print(\"Training Baseline Monolithic Model...\")\n",
    "\n",
    "X_train_mono = train_data.drop(columns=['Tower ID', 'Latency (ms)'])\n",
    "y_train_mono = train_data['Latency (ms)'].values\n",
    "\n",
    "X_test_mono = test_data.drop(columns=['Tower ID', 'Latency (ms)'])\n",
    "y_test_mono = test_data['Latency (ms)'].values\n",
    "\n",
    "print(f\"Monolithic training features shape: {X_train_mono.shape}\")\n",
    "print(f\"Monolithic testing features shape: {X_test_mono.shape}\")\n",
    "\n",
    "# Train monolithic model\n",
    "monolithic_model = MonolithicModel(random_state=RANDOM_STATE)\n",
    "monolithic_model.train(X_train_mono, y_train_mono)\n",
    "\n",
    "print(\"Monolithic model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Monolithic Model\n",
    "pred_mono_train = monolithic_model.predict(X_train_mono)\n",
    "pred_mono_test = monolithic_model.predict(X_test_mono)\n",
    "\n",
    "# Calculate metrics\n",
    "mono_train_metrics = evaluator.calculate_metrics(y_train_mono, pred_mono_train)\n",
    "mono_test_metrics = evaluator.calculate_metrics(y_test_mono, pred_mono_test)\n",
    "\n",
    "print(\"Monolithic Model Performance:\")\n",
    "print(f\"Training - MAE: {mono_train_metrics['MAE']:.3f}, RMSE: {mono_train_metrics['RMSE']:.3f}, R²: {mono_train_metrics['R2_Score']:.3f}\")\n",
    "print(f\"Testing  - MAE: {mono_test_metrics['MAE']:.3f}, RMSE: {mono_test_metrics['RMSE']:.3f}, R²: {mono_test_metrics['R2_Score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "## 6. Performance Evaluation\n",
    "\n",
    "Let's compare all models and analyze their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance comparison\n",
    "results = {\n",
    "    'Infrastructure Model (A)': infra_test_metrics,\n",
    "    'User Behavior Model (B)': user_test_metrics,\n",
    "    'Fusion Model': fusion_test_metrics,\n",
    "    'Monolithic Model': mono_test_metrics\n",
    "}\n",
    "\n",
    "# Create performance comparison table\n",
    "comparison_data = []\n",
    "for model_name, metrics in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'MAE': metrics['MAE'],\n",
    "        'RMSE': metrics['RMSE'],\n",
    "        'R²': metrics['R2_Score'],\n",
    "        'MAPE': metrics.get('MAPE', 0)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('R²', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16)\n",
    "\n",
    "models = comparison_df['Model']\n",
    "colors = ['skyblue', 'lightgreen', 'salmon', 'gold']\n",
    "\n",
    "# MAE comparison\n",
    "axes[0].bar(models, comparison_df['MAE'], color=colors, alpha=0.7)\n",
    "axes[0].set_title('Mean Absolute Error (MAE)')\n",
    "axes[0].set_ylabel('MAE')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[1].bar(models, comparison_df['RMSE'], color=colors, alpha=0.7)\n",
    "axes[1].set_title('Root Mean Square Error (RMSE)')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# R² comparison\n",
    "axes[2].bar(models, comparison_df['R²'], color=colors, alpha=0.7)\n",
    "axes[2].set_title('R² Score')\n",
    "axes[2].set_ylabel('R² Score')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Prediction vs Actual Comparison for All Models', fontsize=16)\n",
    "\n",
    "predictions = {\n",
    "    'Infrastructure Model (A)': pred_infra_test,\n",
    "    'User Behavior Model (B)': pred_user_test,\n",
    "    'Fusion Model': pred_fusion_test,\n",
    "    'Monolithic Model': pred_mono_test\n",
    "}\n",
    "\n",
    "y_actual = y_test_combined\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "\n",
    "for i, (model_name, pred) in enumerate(predictions.items()):\n",
    "    row, col = i // 2, i % 2\n",
    "    r2 = results[model_name]['R2_Score']\n",
    "    \n",
    "    axes[row, col].scatter(y_actual, pred, alpha=0.6, color=colors[i])\n",
    "    axes[row, col].plot([y_actual.min(), y_actual.max()], \n",
    "                        [y_actual.min(), y_actual.max()], 'r--', lw=2)\n",
    "    axes[row, col].set_title(f'{model_name}\\nR² = {r2:.3f}')\n",
    "    axes[row, col].set_xlabel('Actual Latency (ms)')\n",
    "    axes[row, col].set_ylabel('Predicted Latency (ms)')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analysis\"></a>\n",
    "## 7. Results Analysis\n",
    "\n",
    "Let's analyze the results and provide insights about the vertical partitioning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed analysis\n",
    "best_model = comparison_df.iloc[0]\n",
    "print(f\"VERTICAL PARTITIONING ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nBest Performing Model: {best_model['Model']}\")\n",
    "print(f\"  - R² Score: {best_model['R²']:.4f}\")\n",
    "print(f\"  - MAE: {best_model['MAE']:.4f}\")\n",
    "print(f\"  - RMSE: {best_model['RMSE']:.4f}\")\n",
    "\n",
    "# Performance interpretation\n",
    "best_r2 = best_model['R²']\n",
    "if best_r2 > 0.8:\n",
    "    interpretation = \"Excellent model performance achieved (R² > 0.8)\"\n",
    "elif best_r2 > 0.6:\n",
    "    interpretation = \"Good model performance achieved (R² > 0.6)\"\n",
    "elif best_r2 > 0.4:\n",
    "    interpretation = \"Moderate model performance achieved (R² > 0.4)\"\n",
    "else:\n",
    "    interpretation = \"Model performance needs improvement (R² < 0.4)\"\n",
    "\n",
    "print(f\"\\nPerformance Interpretation: {interpretation}\")\n",
    "\n",
    "# Compare fusion vs individual models\n",
    "fusion_r2 = results['Fusion Model']['R2_Score']\n",
    "infra_r2 = results['Infrastructure Model (A)']['R2_Score']\n",
    "user_r2 = results['User Behavior Model (B)']['R2_Score']\n",
    "mono_r2 = results['Monolithic Model']['R2_Score']\n",
    "\n",
    "print(f\"\\nVertical Partitioning Analysis:\")\n",
    "if fusion_r2 > max(infra_r2, user_r2):\n",
    "    print(\"✓ Fusion model outperforms individual models\")\n",
    "    print(\"  This indicates successful feature complementarity in vertical partitioning\")\n",
    "else:\n",
    "    print(\"⚠ Individual models perform better than fusion\")\n",
    "    print(\"  This suggests potential overfitting or suboptimal fusion strategy\")\n",
    "\n",
    "if fusion_r2 > mono_r2:\n",
    "    print(\"✓ Vertical partitioning approach outperforms monolithic model\")\n",
    "    improvement = ((fusion_r2 - mono_r2) / mono_r2) * 100\n",
    "    print(f\"  Performance improvement: {improvement:.2f}%\")\n",
    "else:\n",
    "    print(\"⚠ Monolithic model outperforms vertical partitioning\")\n",
    "    decline = ((mono_r2 - fusion_r2) / mono_r2) * 100\n",
    "    print(f\"  Performance decline: {decline:.2f}%\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(f\"\\nFeature Group Analysis:\")\n",
    "print(f\"  Infrastructure Features (Model A) R²: {infra_r2:.4f}\")\n",
    "print(f\"  User Behavior Features (Model B) R²: {user_r2:.4f}\")\n",
    "\n",
    "if infra_r2 > user_r2:\n",
    "    print(\"  → Infrastructure features are more predictive of network latency\")\n",
    "else:\n",
    "    print(\"  → User behavior features are more predictive of network latency\")\n",
    "\n",
    "# Fusion weights analysis\n",
    "if weights:\n",
    "    print(f\"\\nFusion Strategy Analysis:\")\n",
    "    print(f\"  Infrastructure weight: {weights['infrastructure']:.3f}\")\n",
    "    print(f\"  User behavior weight: {weights['user_behavior']:.3f}\")\n",
    "    \n",
    "    if weights['infrastructure'] > weights['user_behavior']:\n",
    "        print(\"  → Fusion model relies more heavily on infrastructure features\")\n",
    "    else:\n",
    "        print(\"  → Fusion model relies more heavily on user behavior features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "print(\"\\nERROR ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Calculate residuals for best model\n",
    "if best_model['Model'] == 'Fusion Model':\n",
    "    residuals = y_test_combined - pred_fusion_test\n",
    "elif best_model['Model'] == 'Monolithic Model':\n",
    "    residuals = y_test_mono - pred_mono_test\n",
    "elif best_model['Model'] == 'Infrastructure Model (A)':\n",
    "    residuals = y_test_infra - pred_infra_test\n",
    "else:\n",
    "    residuals = y_test_user - pred_user_test\n",
    "\n",
    "print(f\"Residual Statistics for {best_model['Model']}:\")\n",
    "print(f\"  Mean: {np.mean(residuals):.4f}\")\n",
    "print(f\"  Std: {np.std(residuals):.4f}\")\n",
    "print(f\"  Min: {np.min(residuals):.4f}\")\n",
    "print(f\"  Max: {np.max(residuals):.4f}\")\n",
    "\n",
    "# Residual plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(range(len(residuals)), residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title(f'Residuals Plot - {best_model[\"Model\"]}')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Residuals')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(residuals, bins=20, alpha=0.7, color='skyblue')\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusions\"></a>\n",
    "## 8. Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Vertical Partitioning Effectiveness**: The vertical partitioning approach successfully separated features into meaningful groups (infrastructure vs. user behavior).\n",
    "\n",
    "2. **Model Performance**: Individual models showed different strengths in predicting network latency based on their specialized feature sets.\n",
    "\n",
    "3. **Fusion Strategy**: The fusion model combined predictions from both specialized models, potentially capturing complementary information.\n",
    "\n",
    "4. **Comparison with Baseline**: The comparison with the monolithic model provides insights into whether feature specialization improves prediction accuracy.\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "- **Feature Engineering**: Consider additional feature transformations or interactions\n",
    "- **Fusion Strategies**: Experiment with different fusion approaches (stacking, meta-learning)\n",
    "- **Model Selection**: Evaluate different algorithms for each specialized model\n",
    "- **Hyperparameter Tuning**: Optimize model parameters for better performance\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Implement horizontal partitioning approach for geographical-based modeling\n",
    "2. Compare vertical vs. horizontal partitioning strategies\n",
    "3. Deploy the best-performing model for production use\n",
    "4. Monitor model performance over time and retrain as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for future reference\n",
    "print(\"Saving results...\")\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv('vertical_partitioning_notebook_results.csv', index=False)\n",
    "\n",
    "# Save detailed results\n",
    "with open('vertical_partitioning_notebook_summary.txt', 'w') as f:\n",
    "    f.write(\"VERTICAL PARTITIONING NOTEBOOK RESULTS\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Best Performing Model: {best_model['Model']}\\n\")\n",
    "    f.write(f\"R² Score: {best_model['R²']:.4f}\\n\")\n",
    "    f.write(f\"MAE: {best_model['MAE']:.4f}\\n\")\n",
    "    f.write(f\"RMSE: {best_model['RMSE']:.4f}\\n\\n\")\n",
    "    f.write(\"DETAILED RESULTS:\\n\")\n",
    "    f.write(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"Results saved to:\")\n",
    "print(\"  - vertical_partitioning_notebook_results.csv\")\n",
    "print(\"  - vertical_partitioning_notebook_summary.txt\")\n",
    "print(\"\\nNotebook execution completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}